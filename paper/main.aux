\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{o1,r1}
\citation{geminithinking}
\citation{zhou2023lima}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:intro}{{1}{1}{}{section.1}{}}
\newlabel{sec:intro@cref}{{[section][1][]1}{[1][1][]1}{}{}{}}
\citation{numina_math_datasets}
\citation{huang2024olympicarenabenchmarkingmultidisciplinecognitive}
\citation{gao2024omnimathuniversalolympiadlevel}
\citation{zhong2023agievalhumancentricbenchmarkevaluating}
\citation{geminithinking}
\citation{qwen2024qwen25technicalreport}
\@writefile{toc}{\contentsline {section}{\numberline {2}Reasoning data curation to create \textbf  {CoR-1K}{}}{2}{section.2}\protected@file@percent }
\newlabel{sec:data}{{2}{2}{}{section.2}{}}
\newlabel{sec:data@cref}{{[section][2][]2}{[1][2][]2}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Initial collection of 59K samples}{2}{subsection.2.1}\protected@file@percent }
\newlabel{sec:59K}{{2.1}{2}{}{subsection.2.1}{}}
\newlabel{sec:59K@cref}{{[subsection][1][2]2.1}{[1][2][]2}{}{}{}}
\@writefile{toc}{\contentsline {paragraph}{Curation of existing datasets}{2}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Final selection of 1K samples}{2}{subsection.2.2}\protected@file@percent }
\newlabel{sec:selection-criteria}{{2.2}{2}{}{subsection.2.2}{}}
\newlabel{sec:selection-criteria@cref}{{[subsection][2][2]2.2}{[1][2][]2}{}{}{}}
\@writefile{toc}{\contentsline {paragraph}{Quality}{2}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Difficulty}{2}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Diversity}{2}{subsection.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {\textbf  {CoR-1K}{} and \textbf  {CoR-32B}{}.} \textit  {(left)}~\textbf  {CoR-1K}{} is a dataset of 1,000 high-quality, diverse, and difficult questions with reasoning traces. \textit  {(right)}~\textbf  {CoR-32B}{}, a 32B parameter model finetuned on \textbf  {CoR-1K}{} is on the sample-efficiency frontier. See \autoref {tab:perf} for details on other models.}}{3}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:s1k-bar}{{1}{3}{\textbf {\data {} and \model {}.} \textit {(left)}~\data {} is a dataset of 1,000 high-quality, diverse, and difficult questions with reasoning traces. \textit {(right)}~\model {}, a 32B parameter model finetuned on \data {} is on the sample-efficiency frontier. See \autoref {tab:perf} for details on other models}{figure.caption.1}{}}
\newlabel{fig:s1k-bar@cref}{{[figure][1][]1}{[1][2][]3}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Chain of Reward with GRPO}{3}{section.3}\protected@file@percent }
\newlabel{sec:method}{{3}{3}{}{section.3}{}}
\newlabel{sec:method@cref}{{[section][3][]3}{[1][3][]3}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Problem Formalization}{3}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Reward Decomposition}{3}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Total Reward}{3}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{External Reward}{3}{equation.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Intrinsic Reward}{3}{equation.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Reward Chain}{3}{equation.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Endogenous Self-Evaluation}{3}{subsection.3.3}\protected@file@percent }
\newlabel{sec:endogenous}{{3.3}{3}{}{subsection.3.3}{}}
\newlabel{sec:endogenous@cref}{{[subsection][3][3]3.3}{[1][3][]3}{}{}{}}
\@writefile{toc}{\contentsline {paragraph}{Self-Rating Generation}{3}{subsection.3.3}\protected@file@percent }
\citation{aime}
\citation{hendrycks2021measuringmathematicalproblemsolving}
\citation{lightman2023letsverifystepstep}
\citation{rein2023gpqagraduatelevelgoogleproofqa}
\citation{o1}
\citation{eval-harness,biderman2024lessons}
\@writefile{toc}{\contentsline {paragraph}{Self-Rating Quality Reward}{4}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Group Relative Policy Optimization (GRPO)}{4}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Algorithm}{4}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Objective Function}{4}{Item.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Theoretical Properties}{4}{subsection.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{4}{section.4}\protected@file@percent }
\newlabel{sec:results}{{4}{4}{}{section.4}{}}
\newlabel{sec:results@cref}{{[section][4][]4}{[1][4][]4}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Setup}{4}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Training}{4}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Evaluation}{4}{subsection.4.1}\protected@file@percent }
\citation{o1}
\citation{r1}
\citation{qwq-32b-preview}
\citation{sky_t1}
\citation{bespoke_stratos}
\citation{geminithinking}
\citation{qwen2024qwen25technicalreport,qwq-32b-preview,o1,r1,bespoke_stratos,sky_t1}
\citation{qwen2024qwen25technicalreport,qwq-32b-preview,o1,r1,bespoke_stratos,sky_t1}
\citation{r1}
\@writefile{toc}{\contentsline {paragraph}{Other models}{5}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Performance}{5}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Sample-efficiency}{5}{subsection.4.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \textbf  {\textbf  {CoR-32B}{} is a strong open reasoning model.} We evaluate \textbf  {CoR-32B}{}, Qwen, and Gemini (some entries are unknown (N.A.), see \autoref {sec:results}). Other results are from the respective reports~\citep  {qwen2024qwen25technicalreport,qwq-32b-preview,o1,r1,bespoke_stratos,sky_t1}. \# ex. = number examples used for reasoning finetuning.}}{5}{table.caption.2}\protected@file@percent }
\newlabel{tab:perf}{{1}{5}{\textbf {\model {} is a strong open reasoning model.} We evaluate \model {}, Qwen, and Gemini (some entries are unknown (N.A.), see \autoref {sec:results}). Other results are from the respective reports~\citep {qwen2024qwen25technicalreport,qwq-32b-preview,o1,r1,bespoke_stratos,sky_t1}. \# ex. = number examples used for reasoning finetuning}{table.caption.2}{}}
\newlabel{tab:perf@cref}{{[table][1][]1}{[1][5][]5}{}{}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces \textbf  {\textbf  {CoR-1K}{} data ablations.} We budget force (BF) a maximum of around 30,000 thinking tokens for all scores in this table. This performs slightly better than the scores without BF (\autoref {tab:perf}) as it allows the model to finish with a best guess when stuck in an infinite loop. We report 95\% paired bootstrap confidence intervals for differences relative to the \textbf  {CoR-1K}{} model using 10,000 bootstrap samples. E.g., the interval [-13\%, 20\%] means that, with 95\% confidence, the true difference between 59K-full and \textbf  {CoR-1K}{} is between -13\% and +20\%. If the entire interval is negative, e.g. [-27\%, -3\%], we can confidently say that the performance is worse than \textbf  {CoR-1K}{}.}}{5}{table.caption.4}\protected@file@percent }
\newlabel{tab:datablation}{{2}{5}{\textbf {\data {} data ablations.} We budget force (BF) a maximum of around 30,000 thinking tokens for all scores in this table. This performs slightly better than the scores without BF (\autoref {tab:perf}) as it allows the model to finish with a best guess when stuck in an infinite loop. We report 95\% paired bootstrap confidence intervals for differences relative to the \data {} model using 10,000 bootstrap samples. E.g., the interval [-13\%, 20\%] means that, with 95\% confidence, the true difference between 59K-full and \data {} is between -13\% and +20\%. If the entire interval is negative, e.g. [-27\%, -3\%], we can confidently say that the performance is worse than \data {}}{table.caption.4}{}}
\newlabel{tab:datablation@cref}{{[table][2][]2}{[1][5][]5}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textbf  {Example model outputs.} We pick a question from AIME24 (\textit  {left}), MATH500 (\textit  {middle}), and GPQA (\textit  {right}), where our model generates the correct answer. The black text is the prompt, the \textcolor {defaultlightblue}{light blue} text is the reasoning trace, and the \textcolor {defaultblue}{blue} text is the answer of \textbf  {CoR-32B}{}. The gray ellipsis \textcolor [HTML]{808080}{[...]} indicates that the text was trimmed to fit this page, but the generated text is actually longer.}}{6}{figure.caption.3}\protected@file@percent }
\newlabel{fig:ex}{{2}{6}{\textbf {Example model outputs.} We pick a question from AIME24 (\textit {left}), MATH500 (\textit {middle}), and GPQA (\textit {right}), where our model generates the correct answer. The black text is the prompt, the \textcolor {defaultlightblue}{light blue} text is the reasoning trace, and the \textcolor {defaultblue}{blue} text is the answer of \model {}. The gray ellipsis \textcolor [HTML]{808080}{[...]} indicates that the text was trimmed to fit this page, but the generated text is actually longer}{figure.caption.3}{}}
\newlabel{fig:ex@cref}{{[figure][2][]2}{[1][5][]6}{}{}{}}
\citation{schulman2017proximal}
\citation{williams1992simple}
\@writefile{toc}{\contentsline {section}{\numberline {5}Ablations}{7}{section.5}\protected@file@percent }
\newlabel{sec:abl}{{5}{7}{}{section.5}{}}
\newlabel{sec:abl@cref}{{[section][5][]5}{[1][5][]7}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Data Quantity, Diversity, and Difficulty}{7}{subsection.5.1}\protected@file@percent }
\newlabel{sec:dataabl}{{5.1}{7}{}{subsection.5.1}{}}
\newlabel{sec:dataabl@cref}{{[subsection][1][5]5.1}{[1][5][]7}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Theoretical Analysis}{7}{section.6}\protected@file@percent }
\newlabel{sec:theory}{{6}{7}{}{section.6}{}}
\newlabel{sec:theory@cref}{{[section][6][]6}{[1][7][]7}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Convergence Guarantees}{7}{subsection.6.1}\protected@file@percent }
\newlabel{assum:bounded}{{6.1}{7}{Bounded Rewards}{theorem.6.1}{}}
\newlabel{assum:bounded@cref}{{[theorem][1][6]6.1}{[1][7][]7}{}{}{}}
\newlabel{assum:support}{{6.2}{7}{Support Overlap}{theorem.6.2}{}}
\newlabel{assum:support@cref}{{[theorem][2][6]6.2}{[1][7][]7}{}{}{}}
\newlabel{assum:reference}{{6.3}{7}{Reference Regularization}{theorem.6.3}{}}
\newlabel{assum:reference@cref}{{[theorem][3][6]6.3}{[1][7][]7}{}{}{}}
\newlabel{assum:horizon}{{6.4}{7}{Finite Horizon}{theorem.6.4}{}}
\newlabel{assum:horizon@cref}{{[theorem][4][6]6.4}{[1][7][]7}{}{}{}}
\newlabel{assum:lipschitz}{{6.5}{7}{Lipschitz Continuity}{theorem.6.5}{}}
\newlabel{assum:lipschitz@cref}{{[theorem][5][6]6.5}{[1][7][]7}{}{}{}}
\newlabel{thm:improvement}{{6.6}{7}{Policy Improvement with CoR}{theorem.6.6}{}}
\newlabel{thm:improvement@cref}{{[theorem][6][6]6.6}{[1][7][]7}{}{}{}}
\newlabel{thm:unbiased}{{6.7}{7}{Unbiasedness of Group-Normalized Advantages}{theorem.6.7}{}}
\newlabel{thm:unbiased@cref}{{[theorem][7][6]6.7}{[1][7][]7}{}{}{}}
\newlabel{thm:clipped}{{6.8}{7}{Clipped Objective Lower Bounds Surrogate}{theorem.6.8}{}}
\newlabel{thm:clipped@cref}{{[theorem][8][6]6.8}{[1][7][]7}{}{}{}}
\citation{ng1999policy}
\newlabel{thm:convergence}{{6.9}{8}{Convergence to Local Optimum}{theorem.6.9}{}}
\newlabel{thm:convergence@cref}{{[theorem][9][6]6.9}{[1][8][]8}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Endogenous Self-Evaluation Improves Calibration}{8}{subsection.6.2}\protected@file@percent }
\newlabel{prop:calibration}{{6.10}{8}{Calibration Improvement}{theorem.6.10}{}}
\newlabel{prop:calibration@cref}{{[theorem][10][6]6.10}{[1][8][]8}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Potential-Based Reward Shaping}{8}{subsection.6.3}\protected@file@percent }
\newlabel{thm:potential}{{6.11}{8}{Potential-Based Shaping Preserves Optimal Policies}{theorem.6.11}{}}
\newlabel{thm:potential@cref}{{[theorem][11][6]6.11}{[1][8][]8}{}{}{}}
\newlabel{cor:potential}{{6.12}{8}{Process-Quality Features as Potential}{theorem.6.12}{}}
\newlabel{cor:potential@cref}{{[theorem][12][6]6.12}{[1][8][]8}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Extended Bellman Equations with Intrinsic Rewards}{8}{subsection.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Extended Value Function}{8}{subsection.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Extended Bellman Optimality Equation}{8}{equation.22}\protected@file@percent }
\citation{o1}
\citation{r1,k1.5}
\citation{sky_t1,xu2025redstardoesscalinglongcot,bespoke_stratos}
\citation{shao2024deepseekmath}
\citation{pathak2017curiosity,oudeyer2007intrinsic}
\citation{kadavath2022language,xiong2024can}
\citation{guo2017calibration}
\citation{zhou2023lima}
\citation{lightman2023letsverifystepstep,uesato2022solving,luo2024improve}
\citation{wang2024mathshepherd}
\citation{wu2024finegrained}
\citation{ng1999policy}
\citation{madaan2023selfrefine,shinn2023reflexion}
\citation{huang2023large}
\citation{he2024olympiadbenchchallengingbenchmarkpromoting,jain2024livecodebenchholisticcontaminationfree,zhong2023agievalhumancentricbenchmarkevaluating}
\citation{srivastava2023imitation,glazer2024frontiermathbenchmarkevaluatingadvanced,su2024brightrealisticchallengingbenchmark,kim2024llmasaninterviewerstatictestingdynamic,phan2025humanity}
\citation{azerbayev2023llemma,yang2024syntheticcontinuedpretraining}
\citation{yu2023metamath}
\citation{zelikman2022starbootstrappingreasoningreasoning,zelikman2024quietstarlanguagemodelsteach,luo2025wizardmathempoweringmathematicalreasoning,yuan2025agentrtraininglanguagemodel,wu2024thinkingllmsgeneralinstruction}
\citation{wei2023chainofthoughtpromptingelicitsreasoning,yao2024tree,yao2023reactsynergizingreasoningacting,bi2024program,fu2022complexity,zhang2023cumulative,xiang20252reasoningllmslearning,hu2024visual,diao2024activepromptingchainofthoughtlarge}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}Multi-Dimensional Scoring Functions}{9}{subsection.6.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Dimension Scoring Function}{9}{subsection.6.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Examples}{9}{equation.24}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6}Consistency Constraint (Optional Regularizer)}{9}{subsection.6.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Discussion and related work}{9}{section.7}\protected@file@percent }
\newlabel{sec:disc}{{7}{9}{}{section.7}{}}
\newlabel{sec:disc@cref}{{[section][7][]7}{[1][9][]9}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Sample-efficient reasoning}{9}{subsection.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Models}{9}{subsection.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Reward design and process supervision}{9}{subsection.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Benchmarks and methods}{9}{subsection.7.1}\protected@file@percent }
\citation{vonwerra2022trl}
\citation{ouyang2022training}
\citation{bai2022constitutional}
\bibdata{reference}
\bibcite{arora2023llmsadvancedenoughchallenging}{{1}{2023}{{Arora et~al.}}{{}}}
\bibcite{azerbayev2023llemma}{{2}{2023}{{Azerbayev et~al.}}{{}}}
\bibcite{bai2022constitutional}{{3}{2022}{{Bai et~al.}}{{Bai, Kadavath, Kundu, Askell, Kernion, Jones, Chen, Goldie, Mirhoseini, McKinnon, et~al.}}}
\bibcite{bi2024program}{{4}{2024}{{Bi et~al.}}{{}}}
\bibcite{biderman2024lessons}{{5}{2024}{{Biderman et~al.}}{{}}}
\bibcite{chen2023theoremqatheoremdrivenquestionanswering}{{6}{2023}{{Chen et~al.}}{{}}}
\bibcite{geminithinking}{{7}{2024}{{DeepMind}}{{}}}
\bibcite{r1}{{8}{2025}{{DeepSeek-AI}}{{}}}
\bibcite{diao2024activepromptingchainofthoughtlarge}{{9}{2024}{{Diao et~al.}}{{Diao, Wang, Lin, Pan, Liu, and Zhang}}}
\bibcite{dubey2024llama3herdmodels}{{10}{2024}{{Dubey et~al.}}{{}}}
\bibcite{fu2022complexity}{{11}{2022}{{Fu et~al.}}{{}}}
\bibcite{eval-harness}{{12}{2024{a}}{{Gao et~al.}}{{}}}
\bibcite{gao2024omnimathuniversalolympiadlevel}{{13}{2024{b}}{{Gao et~al.}}{{}}}
\bibcite{glazer2024frontiermathbenchmarkevaluatingadvanced}{{14}{2024}{{Glazer et~al.}}{{}}}
\bibcite{groeneveld2024olmo}{{15}{2024}{{Groeneveld et~al.}}{{}}}
\bibcite{guo2017calibration}{{16}{2017}{{Guo et~al.}}{{Guo, Pleiss, Sun, and Weinberger}}}
\bibcite{he2024olympiadbenchchallengingbenchmarkpromoting}{{17}{2024}{{He et~al.}}{{}}}
\bibcite{hendrycks2021measuringmathematicalproblemsolving}{{18}{2021}{{Hendrycks et~al.}}{{Hendrycks, Burns, Kadavath, Arora, Basart, Tang, Song, and Steinhardt}}}
\bibcite{hu2024visual}{{19}{2024}{{Hu et~al.}}{{}}}
\bibcite{huang2023large}{{20}{2023}{{Huang et~al.}}{{Huang, Chen, Mishra, Zheng, Yu, Song, and Zhou}}}
\bibcite{huang2024olympicarenabenchmarkingmultidisciplinecognitive}{{21}{2024}{{Huang et~al.}}{{Huang, Ye, Li, Zheng, Chen, Cheng, Zhang, and Zhang}}}
\bibcite{jain2024livecodebenchholisticcontaminationfree}{{22}{2024}{{Jain et~al.}}{{}}}
\bibcite{kadavath2022language}{{23}{2022}{{Kadavath et~al.}}{{Kadavath, Conerly, Askell, Henighan, Drain, Perez, Schiefer, Hatfield-Dodds, DasSarma, Tyre, et~al.}}}
\bibcite{kim2024llmasaninterviewerstatictestingdynamic}{{24}{2024}{{Kim et~al.}}{{}}}
\bibcite{kwon2023efficientmemorymanagementlarge}{{25}{2023}{{Kwon et~al.}}{{Kwon, Li, Zhuang, Sheng, Zheng, Yu, Gonzalez, Zhang, and Stoica}}}
\bibcite{lightman2023letsverifystepstep}{{26}{2023}{{Lightman et~al.}}{{Lightman, Kosaraju, Burda, Edwards, Baker, Lee, Leike, Schulman, Sutskever, and Wu}}}
\bibcite{ling2017programinductionrationalegeneration}{{27}{2017}{{Ling et~al.}}{{Ling, Grefenstette, Hermann, Ko{\v {c}}isk{\'y}, Blunsom, Dyer, and Hermann}}}
\bibcite{liu2020logiqachallengedatasetmachine}{{28}{2020}{{Liu et~al.}}{{}}}
\bibcite{loshchilov2019decoupled}{{29}{2019}{{Loshchilov \& Hutter}}{{Loshchilov and Hutter}}}
\bibcite{luo2024improve}{{30}{2024}{{Luo et~al.}}{{Luo, Liu, Liu, Phatale, Lara, Li, Shu, Zhu, Meng, Sun, and Rastogi}}}
\bibcite{luo2025wizardmathempoweringmathematicalreasoning}{{31}{2025}{{Luo et~al.}}{{}}}
\bibcite{madaan2023selfrefine}{{32}{2023}{{Madaan et~al.}}{{Madaan, Tandon, Gupta, Hallinan, Gao, Wiegreffe, Alon, Dziri, Prabhumoye, Yang, Gupta, Majumder, Hermann, Welleck, Yazdanbakhsh, and Clark}}}
\bibcite{muennighoff2024olmoeopenmixtureofexpertslanguage}{{33}{2024}{{Muennighoff et~al.}}{{}}}
\bibcite{ng1999policy}{{34}{1999}{{Ng et~al.}}{{Ng, Harada, and Russell}}}
\bibcite{aime}{{35}{2024}{{of~America}}{{}}}
\bibcite{o1}{{36}{2024}{{OpenAI}}{{}}}
\bibcite{oudeyer2007intrinsic}{{37}{2007}{{Oudeyer et~al.}}{{Oudeyer, Kaplan, and Hafner}}}
\bibcite{ouyang2022training}{{38}{2022}{{Ouyang et~al.}}{{Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, et~al.}}}
\bibcite{pathak2017curiosity}{{39}{2017}{{Pathak et~al.}}{{Pathak, Agrawal, Efros, and Darrell}}}
\bibcite{phan2025humanity}{{40}{2025}{{Phan et~al.}}{{}}}
\bibcite{rein2023gpqagraduatelevelgoogleproofqa}{{41}{2023}{{Rein et~al.}}{{}}}
\bibcite{schulman2017proximal}{{42}{2017}{{Schulman et~al.}}{{Schulman, Wolski, Dhariwal, Radford, and Klimov}}}
\bibcite{shao2024deepseekmath}{{43}{2024}{{Shao et~al.}}{{Shao, Wang, Zhu, Xu, Song, Zhang, Li, Wu, and Guo}}}
\bibcite{shi2024languagemodelssolveolympiad}{{44}{2024}{{Shi et~al.}}{{}}}
\bibcite{shinn2023reflexion}{{45}{2023}{{Shinn et~al.}}{{Shinn, Cassano, Gopinath, Narasimhan, and Yao}}}
\bibcite{srivastava2023imitation}{{46}{2023}{{Srivastava et~al.}}{{}}}
\bibcite{su2024brightrealisticchallengingbenchmark}{{47}{2024}{{Su et~al.}}{{Su, Yen, Xia, Shi, Muennighoff, yu~Wang, Liu, Shi, Siegel, Tang, Sun, Yoon, Arik, Chen, and Yu}}}
\bibcite{sun2024scievalmultilevellargelanguage}{{48}{2024}{{Sun et~al.}}{{}}}
\bibcite{bespoke_stratos}{{49}{2024{a}}{{Team}}{{}}}
\bibcite{k1.5}{{50}{2025}{{Team}}{{}}}
\bibcite{numina_math_datasets}{{51}{2024{b}}{{Team}}{{}}}
\bibcite{qwq-32b-preview}{{52}{2024{c}}{{Team}}{{}}}
\bibcite{qwen2024qwen25technicalreport}{{53}{2024}{{Team et~al.}}{{Team, Anton, Chen, Chen, Cheng, Chu, Gao, Gu, Guo, Guo, et~al.}}}
\bibcite{sky_t1}{{54}{2024{d}}{{Team}}{{}}}
\bibcite{uesato2022solving}{{55}{2022}{{Uesato et~al.}}{{Uesato, Kushman, Kumar, Song, Siegel, Wang, Creswell, Irving, and Higgins}}}
\bibcite{vonwerra2022trl}{{56}{2022}{{von Werra et~al.}}{{von Werra, Belkada, Tunstall, Beeching, Thrush, Lambert, and Huang}}}
\bibcite{wang2021lsatprogresschallengescomplex}{{57}{2021}{{Wang et~al.}}{{}}}
\bibcite{wang2024mathshepherd}{{58}{2024}{{Wang et~al.}}{{Wang, Li, Shao, Xu, Dai, Li, Chen, Wu, and Sui}}}
\bibcite{wei2023chainofthoughtpromptingelicitsreasoning}{{59}{2023}{{Wei et~al.}}{{Wei, Wang, Schuurmans, Bosma, Ichter, Xia, Chi, Le, and Zhou}}}
\bibcite{williams1992simple}{{60}{1992}{{Williams}}{{}}}
\bibcite{wu2024finegrained}{{61}{2024{a}}{{Wu et~al.}}{{Wu, Hu, Shi, Dziri, Suhr, Ammanabrolu, Smith, Ostendorf, and Hajishirzi}}}
\bibcite{wu2024thinkingllmsgeneralinstruction}{{62}{2024{b}}{{Wu et~al.}}{{}}}
\bibcite{xiang20252reasoningllmslearning}{{63}{2025}{{Xiang et~al.}}{{}}}
\bibcite{xiong2024can}{{64}{2024}{{Xiong et~al.}}{{Xiong, Hu, Lu, Li, Fu, He, and Hooi}}}
\bibcite{xu2025redstardoesscalinglongcot}{{65}{2025}{{Xu et~al.}}{{Xu, Wu, Wang, Li, Zheng, Chen, Hu, Kang, Ji, Zhang, Guo, Yang, Zhang, and Zhang}}}
\bibcite{yang2024syntheticcontinuedpretraining}{{66}{2024}{{Yang et~al.}}{{}}}
\bibcite{yao2023reactsynergizingreasoningacting}{{67}{2023}{{Yao et~al.}}{{}}}
\bibcite{yao2024tree}{{68}{2024}{{Yao et~al.}}{{}}}
\bibcite{yu2023metamath}{{69}{2023}{{Yu et~al.}}{{}}}
\bibcite{yuan2025agentrtraininglanguagemodel}{{70}{2025}{{Yuan et~al.}}{{}}}
\bibcite{zelikman2022starbootstrappingreasoningreasoning}{{71}{2022}{{Zelikman et~al.}}{{Zelikman, Wu, Mu, and Goodman}}}
\bibcite{zelikman2024quietstarlanguagemodelsteach}{{72}{2024}{{Zelikman et~al.}}{{Zelikman, Harik, Shao, Jayasiri, Haber, and Goodman}}}
\bibcite{zhang2023cumulative}{{73}{2023}{{Zhang et~al.}}{{}}}
\bibcite{zhong2019jecqalegaldomainquestionanswering}{{74}{2019}{{Zhong et~al.}}{{}}}
\bibcite{zhong2023agievalhumancentricbenchmarkevaluating}{{75}{2023}{{Zhong et~al.}}{{Zhong, Cui, Guo, Liang, Lu, Yan, Zheng, Huang, Wang, Wang, et~al.}}}
\bibcite{zhou2023lima}{{76}{2023}{{Zhou et~al.}}{{Zhou, Liu, Xu, Iyer, Sun, Mao, Ma, Efrat, Yu, Yu, et~al.}}}
\bibstyle{icml2025}
\citation{kwon2023efficientmemorymanagementlarge}
\citation{qwen2024qwen25technicalreport}
\citation{qwen2024qwen25technicalreport}
\citation{numina_math_datasets}
\citation{hendrycks2021measuringmathematicalproblemsolving}
\citation{huang2024olympicarenabenchmarkingmultidisciplinecognitive}
\citation{gao2024omnimathuniversalolympiadlevel}
\citation{zhong2023agievalhumancentricbenchmarkevaluating,ling2017programinductionrationalegeneration,hendrycks2021measuringmathematicalproblemsolving,liu2020logiqachallengedatasetmachine,zhong2019jecqalegaldomainquestionanswering,wang2021lsatprogresschallengescomplex}
\citation{he2024olympiadbenchchallengingbenchmarkpromoting}
\citation{chen2023theoremqatheoremdrivenquestionanswering}
\citation{shi2024languagemodelssolveolympiad}
\citation{arora2023llmsadvancedenoughchallenging}
\citation{rein2023gpqagraduatelevelgoogleproofqa}
\citation{sun2024scievalmultilevellargelanguage}
\citation{jain2024livecodebenchholisticcontaminationfree}
\@writefile{toc}{\contentsline {section}{\numberline {A}Evaluation determinism}{16}{appendix.A}\protected@file@percent }
\newlabel{sec:eval-determinism}{{A}{16}{Acknowledgements}{appendix.A}{}}
\newlabel{sec:eval-determinism@cref}{{[section][1][]A}{[1][16][]16}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}\textbf  {CoR-1K}{} details}{16}{appendix.B}\protected@file@percent }
\newlabel{sec:details}{{B}{16}{Acknowledgements}{appendix.B}{}}
\newlabel{sec:details@cref}{{[section][2][]B}{[1][16][]16}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}\textbf  {CoR-1K}{} summary}{16}{subsection.B.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces \textbf  {Summary of our dataset \textbf  {CoR-1K}{}}. Token count measured by the Qwen-2.5 tokenizer. We prompt Claude to produce keywords given several questions from the domain.}}{16}{table.caption.9}\protected@file@percent }
\newlabel{tab:domain_distribution}{{3}{16}{\textbf {Summary of our dataset \data {}}. Token count measured by the Qwen-2.5 tokenizer. We prompt Claude to produce keywords given several questions from the domain}{table.caption.9}{}}
\newlabel{tab:domain_distribution@cref}{{[table][3][]3}{[1][16][]16}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}Dataset composition for full 59K questions}{16}{subsection.B.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces \textbf  {Composition of full 59K questions.} Thinking and response lengths are measured in tokens using the Qwen2.5-32B-Instruct tokenizer~\citep  {qwen2024qwen25technicalreport}. In addition to excluding our evaluation benchmark, AIME24, we also exclude AIME questions from 2022-2023 as we use these 90 questions during our development stage of \textbf  {CoR-32B}{}.}}{17}{table.caption.10}\protected@file@percent }
\newlabel{tab:ds}{{4}{17}{\textbf {Composition of full 59K questions.} Thinking and response lengths are measured in tokens using the Qwen2.5-32B-Instruct tokenizer~\citep {qwen2024qwen25technicalreport}. In addition to excluding our evaluation benchmark, AIME24, we also exclude AIME questions from 2022-2023 as we use these 90 questions during our development stage of \model {}}{table.caption.10}{}}
\newlabel{tab:ds@cref}{{[table][4][]4}{[1][16][]17}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3}\textbf  {CoR-1K}{} grading prompt}{18}{subsection.B.3}\protected@file@percent }
\newlabel{sec:grading}{{B.3}{18}{Acknowledgements}{subsection.B.3}{}}
\newlabel{sec:grading@cref}{{[subsection][3][2]B.3}{[1][18][]18}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {Grading prompt.}}}{18}{figure.caption.11}\protected@file@percent }
\newlabel{fig:grade}{{3}{18}{\textbf {Grading prompt.}}{figure.caption.11}{}}
\newlabel{fig:grade@cref}{{[figure][3][]3}{[1][18][]18}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.4}\textbf  {CoR-1K}{} diversity selection}{18}{subsection.B.4}\protected@file@percent }
\newlabel{sec:algo}{{B.4}{18}{Acknowledgements}{subsection.B.4}{}}
\newlabel{sec:algo@cref}{{[subsection][4][2]B.4}{[1][18][]18}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.5}Decontamination}{18}{subsection.B.5}\protected@file@percent }
\newlabel{sec:decontaminate}{{B.5}{18}{Acknowledgements}{subsection.B.5}{}}
\newlabel{sec:decontaminate@cref}{{[subsection][5][2]B.5}{[1][18][]18}{}{}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Two-stage sampling for \textbf  {CoR-1K}{}}}{19}{algorithm.1}\protected@file@percent }
\newlabel{alg:twostage}{{1}{19}{Two-stage sampling for \data {}}{algorithm.1}{}}
\newlabel{alg:twostage@cref}{{[algorithm][1][]1}{[1][18][]19}{}{}{}}
\citation{qwen2024qwen25technicalreport}
\citation{qwen2024qwen25technicalreport}
\citation{dubey2024llama3herdmodels,groeneveld2024olmo,muennighoff2024olmoeopenmixtureofexpertslanguage}
\citation{loshchilov2019decoupled}
\@writefile{toc}{\contentsline {section}{\numberline {C}Training details}{20}{appendix.C}\protected@file@percent }
\newlabel{sec:details-training}{{C}{20}{Acknowledgements}{appendix.C}{}}
\newlabel{sec:details-training@cref}{{[section][3][]C}{[1][20][]20}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textbf  {Training dynamics of \textbf  {CoR-32B}{} on \textbf  {CoR-1K}{}.}}}{20}{figure.caption.12}\protected@file@percent }
\newlabel{fig:training_metrics}{{4}{20}{\textbf {Training dynamics of \model {} on \data {}.}}{figure.caption.12}{}}
\newlabel{fig:training_metrics@cref}{{[figure][4][]4}{[1][20][]20}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.1}Training Ablations: Sequence length}{20}{subsection.C.1}\protected@file@percent }
\newlabel{sec:trainabl}{{C.1}{20}{Acknowledgements}{subsection.C.1}{}}
\newlabel{sec:trainabl@cref}{{[subsection][1][3]C.1}{[1][20][]20}{}{}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces \textbf  {Training sequence length ablation.} We report ``accuracy / average thinking tokens per sample''; the higher the accuracy and the fewer the thinking tokens (inference cost) the better.}}{20}{table.caption.13}\protected@file@percent }
\newlabel{tab:seqabl}{{5}{20}{\textbf {Training sequence length ablation.} We report ``accuracy / average thinking tokens per sample''; the higher the accuracy and the fewer the thinking tokens (inference cost) the better}{table.caption.13}{}}
\newlabel{tab:seqabl@cref}{{[table][5][]5}{[1][20][]20}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.2}Training Samples}{20}{subsection.C.2}\protected@file@percent }
\newlabel{sec:samples}{{C.2}{20}{Acknowledgements}{subsection.C.2}{}}
\newlabel{sec:samples@cref}{{[subsection][2][3]C.2}{[1][20][]20}{}{}{}}
\gdef \LT@i {\LT@entry 
    {1}{487.8225pt}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces \textbf  {\textbf  {CoR-1K}{} sample stemming from s1-prob.} The black text is the question and the \textcolor {olmoeDarkYellow}{yellow} text is the solution. Based solely on the question we generate the \textcolor {defaultlightblue}{light blue} reasoning trace and \textcolor {defaultblue}{blue} response with Gemini, which we use for training.}}{21}{table.6}\protected@file@percent }
\newlabel{tab:prob}{{6}{21}{\textbf {\data {} sample stemming from s1-prob.} The black text is the question and the \textcolor {olmoeDarkYellow}{yellow} text is the solution. Based solely on the question we generate the \textcolor {defaultlightblue}{light blue} reasoning trace and \textcolor {defaultblue}{blue} response with Gemini, which we use for training}{table.6}{}}
\newlabel{tab:prob@cref}{{[table][6][]6}{[1][21][]21}{}{}{}}
\newlabel{tower}{{27}{21}{Acknowledgements}{equation.27}{}}
\newlabel{tower@cref}{{[equation][27][]27}{[1][21][]21}{}{}{}}
\gdef \LT@ii {\LT@entry 
    {1}{487.8225pt}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces {\textbf  {\textbf  {CoR-1K}{} sample stemming from s1-teasers.} The black text is the question and the \textcolor {olmoeDarkYellow}{yellow} text is the solution. Based solely on the question we generate the \textcolor {defaultlightblue}{light blue} reasoning trace and \textcolor {defaultblue}{blue} response with Gemini, which we use for training.}}}{25}{table.7}\protected@file@percent }
\newlabel{tab:teasers}{{7}{28}{Acknowledgements}{table.7}{}}
\newlabel{tab:teasers@cref}{{[table][7][]7}{[1][25][]28}{}{}{}}
\gdef \LT@iii {\LT@entry 
    {1}{487.8225pt}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces {\textbf  {\textbf  {CoR-1K}{} sample stemming from the astronomy subset of OlympicArena.} The black text is the question and the \textcolor {olmoeDarkYellow}{blue} text is the solution. Based solely on the question we generate the \textcolor {defaultlightblue}{light blue} reasoning trace and \textcolor {defaultblue}{blue} response with Gemini, which we use for training.}}}{29}{table.8}\protected@file@percent }
\newlabel{tab:arena}{{8}{33}{Acknowledgements}{table.8}{}}
\newlabel{tab:arena@cref}{{[table][8][]8}{[1][29][]33}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {D}Version Control}{33}{appendix.D}\protected@file@percent }
\newlabel{sec:vc}{{D}{33}{Acknowledgements}{appendix.D}{}}
\newlabel{sec:vc@cref}{{[section][4][]D}{[1][33][]33}{}{}{}}
\gdef \@abspage@last{33}
