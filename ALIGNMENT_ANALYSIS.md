# CoR è®ºæ–‡ä¸ä»£ç å¯¹é½åˆ†æ

## 1. ç†è®ºå…¬å¼ä¸ä»£ç å®ç°å¯¹æ¯”

### 1.1 å¥–åŠ±åˆ†è§£ (Section 3.1 Method)

**è®ºæ–‡å…¬å¼ (Eq. 2)**:
```
R(c) = R_ext(c) + Î» * R_int(c)
```

**ä»£ç å®ç°** (`train/rewards/calculator.py:232`):
```python
total = external + self.config.lambda_intrinsic * intrinsic
```
âœ… **å®Œå…¨å¯¹é½**

---

### 1.2 å¤–éƒ¨å¥–åŠ± (Eq. 3)

**è®ºæ–‡å…¬å¼**:
```
R_ext(c) = I[y_answer = y_gt]
```

**ä»£ç å®ç°** (`train/rewards/calculator.py:112`):
```python
return 1.0 if answer_clean == gt_clean else 0.0
```
âœ… **å®Œå…¨å¯¹é½**

---

### 1.3 å†…åœ¨å¥–åŠ± (Eq. 4)

**è®ºæ–‡å…¬å¼**:
```
R_int(c) = Î£_{d=1}^{D} w_d * r_d(y_think) + w_self * r_self_rating_quality
```

**ä»£ç å®ç°** (`train/rewards/calculator.py:182-190`):
```python
intrinsic_reward = (
    weighted_intrinsic +  # Î£ w_d * r_d
    self.config.self_rating_weight * self_rating_reward  # w_self * r_self
) / total_weight
```
âœ… **å®Œå…¨å¯¹é½**

---

### 1.4 è‡ªè¯„åˆ†è´¨é‡å¥–åŠ± (Eq. 6-7)

**è®ºæ–‡å…¬å¼**:
```
r_self_rating_quality = (1/D) * Î£ cal_d(self_rating_d/10, actual_quality_d)
cal_d(u, v) = 1 - |u - v|
```

**ä»£ç å®ç°** (`train/rewards/self_rating.py`):
```python
def _calibration_score(self, predicted: float, actual: float) -> float:
    return 1.0 - abs(predicted - actual)

def compute_self_rating_reward(...):
    calibrations = []
    for dim in actual_qualities:
        if dim in self_ratings:
            cal = self._calibration_score(self_ratings[dim], actual_qualities[dim])
            calibrations.append(cal)
    return np.mean(calibrations) if calibrations else 0.5
```
âœ… **å®Œå…¨å¯¹é½**

---

### 1.5 GRPO ä¼˜åŠ¿å‡½æ•° (Eq. 8-10)

**è®ºæ–‡å…¬å¼**:
```
A^(i) = (R(c^(i)) - Î¼_R) / (Ïƒ_R + Îµ)
```

**ä»£ç å®ç°**: ä½¿ç”¨ TRL `GRPOTrainer`ï¼Œå…¶å†…éƒ¨å®ç°äº†æ ‡å‡†åŒ–ä¼˜åŠ¿è®¡ç®—ã€‚
âœ… **é€šè¿‡ TRL åº“å®ç°**

---

### 1.6 GRPO ç›®æ ‡å‡½æ•° (Eq. 11)

**è®ºæ–‡å…¬å¼**:
```
J(Î¸) = E_x[1/N Î£ min(r_i*A^(i), clip(r_i,1-Î´,1+Î´)*A^(i))] - Î²*D_KL(Ï€_Î¸||Ï€_ref)
```

**ä»£ç å®ç°** (`train/grpo.py:244-251`):
```python
trainer = GRPOTrainer(
    model=model,
    ref_model=ref_model,  # For KL penalty
    args=grpo_args,       # Contains Î² (kl_penalty) and Î´ (clip_ratio)
    reward_funcs=reward_fn,
)
```
âœ… **é€šè¿‡ TRL GRPOTrainer å®ç°**

---

## 2. å®éªŒè®¾ç½®å¯¹æ¯”

### 2.1 è®­ç»ƒæµç¨‹

| è®ºæ–‡æè¿° | ä»£ç çŠ¶æ€ |
|---------|---------|
| SFT on Qwen2.5-32B-Instruct with CoR-1K | âœ… `train/sft.py` |
| GRPO training with CoR rewards | âœ… `train/grpo.py` |
| Î» = 1.0 (intrinsic weight) | âœ… é»˜è®¤é…ç½® |
| N = 8 (candidates per group) | âš ï¸ ä»£ç é»˜è®¤ N=4ï¼Œéœ€è°ƒæ•´ |
| Î² = 0.01 (KL penalty) | âš ï¸ éœ€åœ¨ grpo.sh ä¸­é…ç½® |
| Î´ = 0.2 (clipping) | âš ï¸ éœ€åœ¨ grpo.sh ä¸­é…ç½® |
| w_d = 0.2 for each dimension | âš ï¸ ä»£ç é»˜è®¤ w_d=0.25ï¼Œéœ€è°ƒæ•´ |
| w_self = 0.2 | âœ… é»˜è®¤é…ç½® |

### 2.2 æ•°æ®é›†

| è®ºæ–‡æè¿° | ä»£ç çŠ¶æ€ |
|---------|---------|
| CoR-1K: 1000 curated samples | âœ… `local_data/s1K_cor_full` (è§„åˆ™) |
| Self-ratings embedded in thinking | âœ… å·²ç”Ÿæˆ |
| Format: [Self-Rating: Consistency=X/10, ...] | âœ… å·²å®ç° |
| Distilled from Gemini Thinking | åŸå§‹ s1K æ•°æ® |

### 2.3 è¯„ä¼°

| è®ºæ–‡æè¿° | ä»£ç çŠ¶æ€ |
|---------|---------|
| AIME24 (30 problems) | âœ… `eval/lm-evaluation-harness` |
| MATH500 (500 samples) | âœ… å·²é›†æˆ |
| GPQA Diamond (198 questions) | âœ… å·²é›†æˆ |
| Temperature = 0 (greedy) | âœ… é»˜è®¤è®¾ç½® |

---

## 3. ç†è®ºç»“æœå¯¹æ¯” (Section 4: Theory)

### 3.1 å·²å®ç°çš„ç†è®ºä¿è¯

| å®šç† | è®ºæ–‡å†…å®¹ | ä»£ç æ”¯æŒ |
|------|---------|---------|
| Theorem 1 (Policy Improvement) | å†…åœ¨å¥–åŠ±å¼•å¯¼ç­–ç•¥æ”¹è¿› | âœ… GRPOæ¡†æ¶ |
| Theorem 2 (Calibration Improvement) | è‡ªè¯„åˆ†æé«˜æ ¡å‡†åº¦ | âœ… æ ¡å‡†å¥–åŠ± |
| Theorem 3 (Convergence) | æœ‰ç•Œå¥–åŠ±ä¸‹æ”¶æ•› | âœ… å¥–åŠ±åœ¨[0,2]èŒƒå›´ |

### 3.2 éœ€è¦éªŒè¯çš„å‡è®¾

| å‡è®¾ | å†…å®¹ | éªŒè¯çŠ¶æ€ |
|------|------|---------|
| Assumption 1 | æœ‰ç•Œå¥–åŠ± | âœ… ä»£ç å¼ºåˆ¶ |
| Assumption 2 | ç­–ç•¥ç©ºé—´ç´§è‡´ | éšå¼æ»¡è¶³ |
| Assumption 3 | ç¨€ç–å¤–éƒ¨å¥–åŠ± | âœ… äºŒå€¼å¥–åŠ± |

---

## 4. å®éªŒç»“æœå¯¹æ¯”

### 4.1 è®ºæ–‡ä¸»è¦ç»“æœ (Table 1)

| æ¨¡å‹ | è®­ç»ƒæ ·æœ¬ | AIME24 | MATH500 | GPQA |
|-----|---------|--------|---------|------|
| CoR-32B w/o CoR | 1K | 50.0 | 92.6 | 56.6 |
| CoR-32B | 1K | 56.7 | 93.0 | 59.6 |
| **æå‡** | - | **+6.7** | **+0.4** | **+3.0** |

### 4.2 å®éªŒéªŒè¯å¾…åŠ

1. [ ] è¿è¡Œ SFT baseline (w/o CoR)
2. [ ] è¿è¡Œ GRPO + CoR è®­ç»ƒ
3. [ ] åœ¨ AIME24/MATH500/GPQA ä¸Šè¯„ä¼°
4. [ ] å¯¹æ¯” baseline å’Œ CoR ç»“æœ
5. [ ] éªŒè¯æ ¡å‡†åº¦æ”¹è¿›

---

## 5. ä»£ç ä¿®æ­£å»ºè®®

### 5.1 é…ç½®å¯¹é½ (é«˜ä¼˜å…ˆçº§)

```python
# train/rewards/calculator.py - ä¿®æ”¹é»˜è®¤æƒé‡
dimension_weights: Dict[str, float] = field(default_factory=lambda: {
    "consistency": 0.2,    # æ”¹ä¸º 0.2
    "completeness": 0.2,   # æ”¹ä¸º 0.2
    "accuracy": 0.2,       # æ–°å¢
    "clarity": 0.2,        # æ”¹ä¸º 0.2
})

# train/grpo.py - ä¿®æ”¹é»˜è®¤å€™é€‰æ•°
num_generations: int = field(default=8)  # æ”¹ä¸º 8
```

### 5.2 è®­ç»ƒè„šæœ¬å¯¹é½

```bash
# train/grpo.sh - æ·»åŠ è®ºæ–‡å‚æ•°
--num_generations 8 \
--kl_penalty 0.01 \
--clip_ratio 0.2 \
--lambda_intrinsic 1.0 \
```

### 5.3 æ•°æ®é›†å­—æ®µå¯¹é½

æ•°æ®é›†éœ€è¦åŒ…å«ï¼š
- `question`: é—®é¢˜
- `thinking_rated`: å¸¦è‡ªè¯„åˆ†çš„æ€ç»´é“¾
- `answer`: æœ€ç»ˆç­”æ¡ˆ
- `ground_truth`: æ­£ç¡®ç­”æ¡ˆ (ç”¨äºå¤–éƒ¨å¥–åŠ±)

---

## 6. å®éªŒæ‰§è¡Œè®¡åˆ’

### Phase 1: Baseline SFT (1-2å°æ—¶)
```bash
cd s1/train
./sft.sh --dataset local_data/s1K_cor_full
```

### Phase 2: GRPO + CoR (4-8å°æ—¶)
```bash
cd s1/train
./grpo.sh --ref_model ckpts/sft-baseline
```

### Phase 3: è¯„ä¼° (1-2å°æ—¶)
```bash
cd s1/eval
python generate.py --model ckpts/cor-grpo --benchmarks aime24,math500,gpqa
```

### Phase 4: æ¶ˆèå®éªŒ
1. CoR vs vanilla GRPO (æ— å†…åœ¨å¥–åŠ±)
2. ä¸åŒ Î» å€¼å¯¹æ¯”
3. è‡ªè¯„åˆ†è´¨é‡å¥–åŠ±è´¡çŒ®

---

## 7. å½“å‰çŠ¶æ€æ€»ç»“

### âœ… å·²å®Œæˆ
- å¥–åŠ±è®¡ç®—æ¨¡å— (RewardCalculator)
- è‡ªè¯„åˆ†æå–å’Œè¯„ä¼°
- GRPO è®­ç»ƒæ¡†æ¶
- æ•°æ®é›†ç”Ÿæˆ (è§„åˆ™ + DeepSeek)
- æ•°æ®åŠ è½½å·¥å…·

### âš ï¸ éœ€è¦è°ƒæ•´
- é»˜è®¤å‚æ•°å¯¹é½è®ºæ–‡
- grpo.sh è„šæœ¬å®Œå–„
- æ·»åŠ  accuracy ç»´åº¦

### ğŸ”„ è¿›è¡Œä¸­
- DeepSeek å¢å¼ºæ•°æ®é›†ç”Ÿæˆ

### ğŸ“ å¾…éªŒè¯
- å®é™…è®­ç»ƒè¿è¡Œ
- åŸºå‡†æµ‹è¯•ç»“æœ
- è®ºæ–‡è¡¨æ ¼æ•°æ®
