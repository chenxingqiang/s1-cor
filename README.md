# Chain of Reward (CoR) â€” ç†è®ºä¸ä»£ç å¯¹åº”æ–‡æ¡£

<p align="center">
  <strong>ğŸ”— å†…ç”Ÿè‡ªè¯„ä¼°çš„å¼ºåŒ–å­¦ä¹ æ¨ç†æ¡†æ¶</strong>
</p>

<p align="center">
  <a href="#æ ¸å¿ƒåˆ›æ–°">æ ¸å¿ƒåˆ›æ–°</a> â€¢
  <a href="#ç†è®º-ä»£ç å¯¹åº”">ç†è®º-ä»£ç å¯¹åº”</a> â€¢
  <a href="#å¿«é€Ÿå¼€å§‹">å¿«é€Ÿå¼€å§‹</a> â€¢
  <a href="#éªŒè¯é€»è¾‘">éªŒè¯é€»è¾‘</a>
</p>

---

## æ ¸å¿ƒåˆ›æ–°

CoR (Chain of Reward) æ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå…·æœ‰ä¸‰å¤§åˆ›æ–°ï¼š

1. **å†…ç”Ÿè‡ªè¯„ä¼° (Endogenous Self-Evaluation)**: æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¸»åŠ¨ç”Ÿæˆå¤šç»´åº¦è‡ªè¯„åˆ†
2. **CoR-GRPO åŒè€¦åˆ**: å¥–åŠ±ä¿¡å·ä¸ç­–ç•¥ä¼˜åŒ–çš„åŒå‘ååŒè¿›åŒ–
3. **è‡ªåçœå¾ªç¯**: é€šè¿‡è¿­ä»£åçœå®ç°"è¶Šæ¨ç†è¶Šæ™ºèƒ½"

---

## ç†è®º-ä»£ç å¯¹åº”

### ğŸ“ 1. æ ¸å¿ƒå¥–åŠ±å…¬å¼

| ç†è®ºå…¬å¼ | ä»£ç å®ç° |
|---------|---------|
| `R(c) = R_ext(c) + Î»Â·R_int(c)` | `calculator.py:calculate_total_reward()` |

**ç†è®º (theory.md Â§1):**
```
R(c) = R_ext(c) + Î»Â·R_int(c)

- R_ext: å¤–éƒ¨å¥–åŠ±ï¼ˆç­”æ¡ˆæ­£ç¡®æ€§ï¼Œç¨€ç–ï¼‰
- R_int: å†…åœ¨å¥–åŠ±ï¼ˆæ¨ç†è´¨é‡ï¼Œç¨ å¯†ï¼‰
- Î»: å¹³è¡¡æƒé‡ = 1.0
```

**ä»£ç å®ç°:**
```python
# s1-cor/train/rewards/calculator.py:245-260
def calculate_total_reward(self, thinking_chain, answer, ground_truth, ...):
    # å¤–éƒ¨å¥–åŠ±
    external = self.calculate_external_reward(answer, ground_truth, grader_fn)
    
    # å†…åœ¨å¥–åŠ±ï¼ˆå«è‡ªè¯„åˆ†è´¨é‡ï¼‰
    intrinsic, dim_scores = self.calculate_intrinsic_reward(
        thinking_chain,
        include_self_rating=True,
        final_answer_correct=(external > 0.5),
    )
    
    # æ€»å¥–åŠ± = R_ext + Î» * R_int
    total = external + self.config.lambda_intrinsic * intrinsic
    
    return RewardOutput(total_reward=total, ...)
```

---

### ğŸ“Š 2. äº”ç»´åº¦å†…åœ¨å¥–åŠ±

| ç†è®ºå…¬å¼ | ä»£ç å®ç° |
|---------|---------|
| `R_int = Î£ w_dÂ·r_d(y_think)` | `intrinsic.py:IntrinsicRewardCalculator` |

**ç†è®º (theory.md Â§2):**
```
R_int(c) = Î£_{d=1}^5 w_dÂ·r_d(y_think) + w_selfÂ·r_self_rating_quality

ç»´åº¦: Consistency, Completeness, Accuracy, Clarity, Format
æƒé‡: w_d = 0.2 (æ¯ç»´åº¦)
```

**ä»£ç å®ç°:**
```python
# s1-cor/train/rewards/intrinsic.py:353-385
class IntrinsicRewardCalculator:
    DEFAULT_WEIGHTS = {
        "consistency": 0.2,   # é€»è¾‘ä¸€è‡´æ€§
        "completeness": 0.2,  # æ­¥éª¤å®Œæ•´æ€§
        "accuracy": 0.2,      # äº‹å®å‡†ç¡®æ€§
        "clarity": 0.2,       # æ¨ç†æ¸…æ™°åº¦
        "format": 0.2,        # æ ¼å¼æ­£ç¡®æ€§
    }
    
    def __init__(self, weights=None):
        self.weights = weights or self.DEFAULT_WEIGHTS.copy()
        self.reward_functions = {
            "consistency": ConsistencyReward(),
            "completeness": CompletenessReward(),
            "accuracy": AccuracyReward(),
            "clarity": ClarityReward(),
            "format": FormatReward(),
        }
```

**å„ç»´åº¦è¯„åˆ†å‡½æ•°:**

| ç»´åº¦ | ç±» | è¯„ä¼°é€»è¾‘ |
|-----|-----|---------|
| Consistency | `ConsistencyReward` | æ£€æµ‹é€»è¾‘è¯ã€æ­¥éª¤å¼•ç”¨ã€æ— çŸ›ç›¾ |
| Completeness | `CompletenessReward` | æ­¥éª¤æ•°é‡ã€é—®é¢˜è¦†ç›–åº¦ |
| Accuracy | `AccuracyReward` | æ•°å­¦è¡¨è¾¾å¼ã€å…³é”®è¯ä½¿ç”¨ |
| Clarity | `ClarityReward` | ç»“æ„æ ‡è®°ã€å¯è¯»æ€§ |
| Format | `FormatReward` | æ ¼å¼å®Œæ•´æ€§ã€æ‹¬å·åŒ¹é… |

---

### ğŸ¯ 3. è‡ªè¯„åˆ†æ ¡å‡†å¥–åŠ±

| ç†è®ºå…¬å¼ | ä»£ç å®ç° |
|---------|---------|
| `cal_d(u,v) = 1 - |u - v|` | `self_rating.py:compute_calibration()` |

**ç†è®º (theory.md Â§3):**
```
cal_d(u, v) = 1 - |u - v|
r_self = (1/D) Î£ cal_d(self_rating_d/10, actual_d)

- u: æ¨¡å‹è‡ªè¯„åˆ† (å½’ä¸€åŒ–åˆ° 0-1)
- v: å®é™…è´¨é‡åˆ†æ•° (0-1)
- é«˜-é«˜å¯¹é½å¥–åŠ±: +Î± (å½“ u>0.8 ä¸” v>0.8)
```

**ä»£ç å®ç°:**
```python
# s1-cor/train/rewards/self_rating.py:220-249
class SelfRatingEvaluator:
    def compute_calibration(self, self_rating, actual_quality, apply_bonus=True):
        """
        From THEORY.md Definition 8:
        cal_d(u, v) = 1 - |u - v|
        """
        # åŸºç¡€æ ¡å‡†
        cal = 1.0 - abs(self_rating - actual_quality)
        
        # é«˜-é«˜å¯¹é½å¥–åŠ±
        if apply_bonus and self_rating > 0.8 and actual_quality > 0.8:
            cal += self.calibration_bonus  # Î± = 0.2
        
        return cal
```

**è‡ªè¯„åˆ†æå–:**
```python
# s1-cor/train/rewards/self_rating.py:84-146
class SelfRatingExtractor:
    """
    æ”¯æŒæ ¼å¼:
    - [Self-Rating: Consistency=8/10, Completeness=9/10]
    - [è¯„åˆ†: é€»è¾‘ä¸€è‡´æ€§=8/10, æ­¥éª¤å®Œæ•´æ€§=9/10]
    """
    def extract(self, thinking_chain) -> Dict[str, SelfRating]:
        # å°è¯•ç»“æ„åŒ–æ ¼å¼: [Self-Rating: Dim1=X/10, ...]
        match = re.search(r'\[Self-Rating:\s*([^\]]+)\]', thinking_chain)
        if match:
            return self._parse_structured_rating(match.group(1))
        ...
```

---

### ğŸ”„ 4. æ”¹è¿›å¥–åŠ±ï¼ˆè‡ªåçœï¼‰

| ç†è®ºå…¬å¼ | ä»£ç å®ç° |
|---------|---------|
| `R_improve = Q(c_{k+1}) - Q(c_k)` | `intrinsic.py:ImprovementRewardCalculator` |

**ç†è®º (theory.md Â§9):**
```
R_improve(c_k, c_{k+1}) = Q(c_{k+1}) - Q(c_k)

ç´¯ç§¯æ”¹è¿›:
R_total_improve = Î£_{k=0}^{K-1} Î³^k Â· R_improve^{(k)}
```

**ä»£ç å®ç°:**
```python
# s1-cor/train/rewards/intrinsic.py:460-530
class ImprovementRewardCalculator:
    def compute_improvement(self, chain_old, chain_new, **kwargs):
        """
        R_improve = Q(c_new) - Q(c_old)
        """
        q_old = self.compute_quality(chain_old, **kwargs)
        q_new = self.compute_quality(chain_new, **kwargs)
        return q_new - q_old
    
    def compute_cumulative_improvement(self, chain_sequence, gamma=0.9, **kwargs):
        """
        R_total = Î£_{k=0}^{K-1} Î³^k * R_improve(c_k, c_{k+1})
        """
        total = 0.0
        for k in range(len(chain_sequence) - 1):
            improvement = self.compute_improvement(
                chain_sequence[k], 
                chain_sequence[k + 1]
            )
            total += (gamma ** k) * improvement
        return total
```

---

### âš–ï¸ 5. æ”¶æ•›å¥–åŠ±

| ç†è®ºå…¬å¼ | ä»£ç å®ç° |
|---------|---------|
| `R_converge = 1 - |c_{k+1} - c_k|` | `intrinsic.py:ConvergenceRewardCalculator` |

**ç†è®º (design.md Â§2.5):**
```
R_converge = -|c_{k+1} - c_k|  (å½’ä¸€åŒ–å)

é¼“åŠ±æ¨¡å‹æ”¶æ•›è€ŒéæŒ¯è¡
```

**ä»£ç å®ç°:**
```python
# s1-cor/train/rewards/intrinsic.py:533-580
class ConvergenceRewardCalculator:
    def compute_convergence_reward(self, chain_old, chain_new, **kwargs):
        """æ”¶æ•›å¥–åŠ± = 1 - divergence (å½’ä¸€åŒ–)"""
        divergence = self.compute_divergence(chain_old, chain_new, **kwargs)
        return max(0.0, 1.0 - divergence)
    
    def has_converged(self, chain_old, chain_new, threshold=0.1, **kwargs):
        """æ£€æŸ¥æ˜¯å¦å·²æ”¶æ•›"""
        divergence = self.compute_divergence(chain_old, chain_new)
        return divergence < threshold
```

---

### ğŸ”— 6. æ‰©å±•å¥–åŠ±å…¬å¼ï¼ˆå«åçœï¼‰

| ç†è®ºå…¬å¼ | ä»£ç å®ç° |
|---------|---------|
| `R = R_ext + Î»Â·R_int + Î¼Â·R_improve + Î½Â·R_converge` | `calculator.py:calculate_reflection_reward()` |

**ç†è®º (theory.md Â§11):**
```
R_CoR_Reflect = R_ext + Î»Â·R_int + Î¼Â·R_improve + Î½Â·R_converge

å‚æ•°:
- Î» = 1.0 (å†…åœ¨æƒé‡)
- Î¼ = 0.5 (æ”¹è¿›æƒé‡)
- Î½ = 0.1 (æ”¶æ•›æƒé‡)
```

**ä»£ç å®ç°:**
```python
# s1-cor/train/rewards/calculator.py:275-350
def calculate_reflection_reward(self, chain_sequence, final_answer, ground_truth, ...):
    """
    Extended formula: R = R_ext + Î»Â·R_int + Î¼Â·R_improve + Î½Â·R_converge
    """
    # 1. å¤–éƒ¨å¥–åŠ±
    external = self.calculate_external_reward(final_answer, ground_truth)
    
    # 2. å†…åœ¨å¥–åŠ±
    intrinsic, dim_scores = self.calculate_intrinsic_reward(final_chain, ...)
    
    # 3. æ”¹è¿›å¥–åŠ±ï¼ˆç´¯ç§¯ï¼‰
    improvement = self.improvement_calculator.compute_cumulative_improvement(
        chain_sequence, gamma=self.config.improvement_discount
    )
    
    # 4. æ”¶æ•›å¥–åŠ±
    convergence = self.convergence_calculator.compute_convergence_reward(
        chain_sequence[-2], chain_sequence[-1]
    )
    
    # æ€»å¥–åŠ±
    total = (
        external +
        self.config.lambda_intrinsic * intrinsic +      # Î» = 1.0
        self.config.improvement_weight * improvement +   # Î¼ = 0.5
        self.config.convergence_weight * convergence     # Î½ = 0.1
    )
```

---

### âš™ï¸ 7. é…ç½®å‚æ•°

| ç†è®ºå‚æ•° | ä»£ç é…ç½® | å€¼ |
|---------|---------|---|
| Î» (intrinsic) | `RewardConfig.lambda_intrinsic` | 1.0 |
| Î¼ (improve) | `RewardConfig.improvement_weight` | 0.5 |
| Î½ (converge) | `RewardConfig.convergence_weight` | 0.1 |
| K (rounds) | `RewardConfig.max_reflection_rounds` | 3 |
| Î± (bonus) | `RewardConfig.calibration_bonus` | 0.2 |
| N (candidates) | `CoRTrainingConfig.num_generations` | 8 |

**ä»£ç :**
```python
# s1-cor/train/rewards/calculator.py:23-55
@dataclass
class RewardConfig:
    lambda_intrinsic: float = 1.0       # Î»
    improvement_weight: float = 0.5      # Î¼
    convergence_weight: float = 0.1      # Î½
    max_reflection_rounds: int = 3       # K
    calibration_bonus: float = 0.2       # Î±
    
    dimension_weights: Dict[str, float] = field(default_factory=lambda: {
        "consistency": 0.2,
        "completeness": 0.2,
        "accuracy": 0.2,
        "clarity": 0.2,
        "format": 0.2,
    })
```

---

### ğŸ“ 8. GRPO è®­ç»ƒé›†æˆ

| ç†è®ºç®—æ³• | ä»£ç å®ç° |
|---------|---------|
| Algorithm 1: CoR-Reflect | `grpo.py:create_reward_fn()` |

**ç†è®º (theory.md Â§13):**
```
for each batch:
    c^{(0)} ~ Ï€_Î¸(Â·|x)                          # åˆå§‹ç”Ÿæˆ
    for k = 0 to K-1:                           # å¤šè½®åçœ
        self_rating_k = extract(c^{(k)})
        c^{(k+1)} ~ Ï€_Î¸(Â·| x, reflection)
        R_improve^{(k)} = Q(c^{(k+1)}) - Q(c^{(k)})
    R_total = R_ext + Î»Â·R_int + Î¼Â·Î£_k R_improve^{(k)}
    Î¸ â† Î¸ + Î±Â·âˆ‡_Î¸ J(Î¸)                          # GRPO æ›´æ–°
```

**ä»£ç å®ç°:**
```python
# s1-cor/train/grpo.py:89-170
def create_reward_fn(config, enable_logging=True):
    calculator = RewardCalculator(reward_config)
    
    def reward_fn(completions: List[str], **kwargs) -> List[float]:
        for i, completion in enumerate(completions):
            # æå–åçœè½®æ¬¡
            chain_sequence = extract_reflection_rounds(completion)
            
            if len(chain_sequence) > 1 and config.enable_reflection:
                # å¤šè½®åçœï¼šä½¿ç”¨æ‰©å±•å¥–åŠ±
                output = calculator.calculate_reflection_reward(
                    chain_sequence, answer, gt
                )
                rewards.append(output.total_reward)
            else:
                # å•è½®ï¼šæ ‡å‡† CoR å¥–åŠ±
                output = calculator.calculate_total_reward(
                    thinking, answer, gt
                )
                rewards.append(output.total_reward)
        
        return rewards
    
    return reward_fn
```

---

## ğŸ“ é¡¹ç›®ç»“æ„

```
s1-cor/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ rewards/
â”‚   â”‚   â”œâ”€â”€ __init__.py           # æ¨¡å—å¯¼å‡º
â”‚   â”‚   â”œâ”€â”€ calculator.py         # RewardCalculator (æ ¸å¿ƒ)
â”‚   â”‚   â”œâ”€â”€ self_rating.py        # è‡ªè¯„åˆ†æå–ä¸æ ¡å‡†
â”‚   â”‚   â”œâ”€â”€ intrinsic.py          # 5ç»´åº¦è¯„åˆ† + åçœå¥–åŠ±
â”‚   â”‚   â””â”€â”€ training_logger.py    # è®­ç»ƒæ—¥å¿—è¿½è¸ª
â”‚   â”‚
â”‚   â”œâ”€â”€ grpo.py                   # GRPO è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ sft_small.py              # SFT è®­ç»ƒè„šæœ¬
â”‚   â””â”€â”€ validate_cor_logic.py     # CoR é€»è¾‘éªŒè¯
â”‚
â”œâ”€â”€ local_data/                   # æœ¬åœ°æ•°æ®é›†
â”œâ”€â”€ theory.md                     # æ•°å­¦ç†è®º
â”œâ”€â”€ design.md                     # è®¾è®¡æ–‡æ¡£
â””â”€â”€ README.md                     # æœ¬æ–‡æ¡£
```

---

## å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
pip install torch transformers datasets trl accelerate
```

### 2. éªŒè¯ CoR é€»è¾‘

```bash
cd s1-cor/train
python validate_cor_logic.py --dataset hf --samples 5
```

è¾“å‡ºç¤ºä¾‹ï¼š
```
ğŸ”¬ VALIDATING SAMPLE 0
----------------------------------------
1ï¸âƒ£  SELF-RATING EXTRACTION
   âœ… Found 5 self-ratings:
      â€¢ consistency: 4.0/10 (normalized: 0.40)
      â€¢ completeness: 5.0/10 (normalized: 0.50)
      ...

2ï¸âƒ£  INTRINSIC DIMENSION SCORES
   Consistency : [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 1.000
   Completeness: [â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 0.250
   ...

3ï¸âƒ£  SELF-RATING CALIBRATION
   ğŸ“Š Average calibration: 0.680
   ğŸ‘ Good calibration

4ï¸âƒ£  TOTAL REWARD CALCULATION
   R_ext (external):  1.0000  âœ…
   R_int (intrinsic): 0.6139
   R_total:           1.6139
```

### 3. è¿è¡Œ SFT è®­ç»ƒ

```bash
python train/sft_small.py --model_size 0.5B --dataset hf --push_to_hub
```

### 4. è¿è¡Œ GRPO è®­ç»ƒ

```bash
bash train/grpo.sh
```

---

## éªŒè¯é€»è¾‘

### ç†è®ºä¿è¯

| å®šç† | å«ä¹‰ | éªŒè¯æ–¹å¼ |
|-----|------|---------|
| **ååŒå¢ç›Š** | åŒè€¦åˆè¿›åŒ–å¿«äºç‹¬ç«‹ç»„ä»¶ | å¯¹æ¯”å®éªŒ |
| **æ”¶æ•›ä¿è¯** | å‹ç¼©æ˜ å°„æ”¶æ•›åˆ°ä¸åŠ¨ç‚¹ | åçœè½®æ¬¡è¿½è¸ª |
| **å•è°ƒæ”¹è¿›** | æ¯è½®åçœè´¨é‡æå‡ | R_improve > 0 |
| **Lyapunov ç¨³å®š** | ç³»ç»Ÿèƒ½é‡æŒç»­ä¸‹é™ | è®­ç»ƒæ›²çº¿ç›‘æ§ |

### æ—¥å¿—è¿½è¸ª

è®­ç»ƒæ—¶ä¼šè¾“å‡ºè¯¦ç»†æ—¥å¿—ï¼š
```
ğŸ“Š CoR Reward Log | Step 100 | Sample: sample_0...
======================================================================
ğŸ¯ REWARD BREAKDOWN:
   R_ext (external)     = 1.0000  âœ…
   R_int (intrinsic)    = 0.6139
   R_improve (reflect)  = 0.1500
   R_converge (stable)  = 0.0800
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   R_total              = 1.8439

ğŸ“ DIMENSION SCORES (5-dim):
   Consistency : [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 1.000
   Completeness: [â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 0.250
   Accuracy    : [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘] 0.530
   Clarity     : [â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘] 0.400
   Format      : [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘] 0.800

ğŸ” SELF-RATING CALIBRATION:
   âœ… Self-ratings detected
   Calibration quality: 0.7034
```

---

## æ•°æ®æ ¼å¼

### å•è½®æ¨ç†ï¼ˆå½“å‰ï¼‰
```
<thinking>
...æ¨ç†æ­¥éª¤...
[Self-Rating: Consistency=7/10, Completeness=8/10, Accuracy=6/10, Clarity=7/10]
</thinking>
<answer>æœ€ç»ˆç­”æ¡ˆ</answer>
```

### å¤šè½®åçœï¼ˆæ‰©å±•ï¼‰
```
[Round 1]
<thinking>...åˆå§‹æ¨ç†...</thinking>
[Self-Rating: Consistency=4/10, Accuracy=3/10, ...]

[Reflection]
å‡†ç¡®æ€§è¾ƒä½ (3/10)ã€‚æ­¥éª¤ 2 å­˜åœ¨é”™è¯¯...

[Round 2]
<thinking>...ä¿®æ­£åçš„æ¨ç†...</thinking>
[Self-Rating: Consistency=8/10, Accuracy=9/10, ...]

[Convergence: Î”=+4.5, Stop=True]

<answer>æœ€ç»ˆç­”æ¡ˆ</answer>
```

---

## å¼•ç”¨

å¦‚æœæ‚¨ä½¿ç”¨æœ¬é¡¹ç›®ï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@misc{cor2024,
  title={Chain of Reward: Endogenous Self-Evaluation for Reasoning},
  author={...},
  year={2024},
  howpublished={\url{https://github.com/chenxingqiang/s1-cor}}
}
```

---

## è®¸å¯è¯

MIT License

---

<p align="center">
  <strong>ğŸ¯ CoR: è®©æ¨¡å‹è¶Šæ¨ç†è¶Šæ™ºèƒ½</strong>
</p>
